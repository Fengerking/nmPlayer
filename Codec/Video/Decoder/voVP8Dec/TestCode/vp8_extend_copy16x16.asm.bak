;
;  Copyright (c) 2011 VisualOn. All Rights Reserved.
;
;

		AREA |.text|, CODE, READONLY
    	EXPORT  vo_B_LD_PRED_neon
    	EXPORT  vo_B_DC_PRED_neon
    	EXPORT  vo_B_TM_PRED_neon
;vo_B_TM_PRED_neon(BYTE*Above,BYTE*Left,BYTE top_left,BYTE*predictor);
;vo_B_DC_PRED_neon(BYTE*Above,BYTE*left,BYTE* predictor); 
;vo_B_LD_PRED_neon(BYTE*Above,BYTE* predictor);

vo_B_TM_PRED_neon PROC
	stmdb       sp!, {lr}
	vdup.u16        d10,r2  
	
	mov    r2,#16  
	vld1.u32         {d0[0]}, [r0]   ; d0[0]: A0  A1  A2  A3	
	vld1.u32         {d0[1]}, [r1]   ; d0[1]: L0  L1  L2  L3
	
	vshll.u8           q0,d0,#0
	
	vsub.s16	     d0,d0,d10         
	
	vdup.s16           d2,d1[0]       ; L0 L0 L0 L0
	vdup.s16           d4,d1[1]       ; L1 L1 L1 L1
	vdup.s16           d6,d1[2]       ; L2 L2 L2 L2
	vdup.s16           d8,d1[3]       ; L3 L3 L3 L3
	
	vadd.s16          d2,d2,d0
	vadd.s16          d4,d4,d0
	vadd.s16          d6,d6,d0
	vadd.s16          d8,d8,d0	
	
	vqmovun.s16     d2, q1
	vqmovun.s16     d4, q2
	vqmovun.s16     d6, q3
	vqmovun.s16     d8, q4
	
	vst1.u32        {d2[0]},[r3],r2  
	vst1.u32        {d4[0]},[r3],r2
	vst1.u32        {d6[0]},[r3],r2
	vst1.u32        {d8[0]},[r3],r2		
	
	ldmia       sp!, {pc}
	ENDP 
vo_B_DC_PRED_neon PROC
	stmdb       sp!, {lr}	
	ldr             r3, =DATA2
	vld1.u32         {d8}, [r3]	
	mov             r3,#16	
	vld1.u32         {d0[0]}, [r0]   ; d0: A0  A1  A2  A3	
	vld1.u32         {d0[1]}, [r1]   ; d0: L0  L1  L2  L3	
	vpaddl.u8         d0,d0	
	vpaddl.u16        d0,d0	
	vpaddl.u32        d0,d0		
	vadd.u32          d0,d0,d8
	vshr.u32          d0,d0,#3	
	vdup.u8           d0,d0[0]		
	vst1.u32        {d0[0]},[r2],r3  
	vst1.u32        {d0[0]},[r2],r3
	vst1.u32        {d0[0]},[r2],r3
	vst1.u32        {d0[0]},[r2],r3	
	ldmia       sp!, {pc}
	ENDP 
vo_B_LD_PRED_neon PROC
	
	stmdb       sp!, {lr}
	vld1.u8         {d0}, [r0]      ;d0:  p0 		p1		p2		p3 		p4 		p5 		p6 		p7
	
	ldr             r2, =DATA
	vld1.u8         {d11}, [r2] 
	
	mov  r3,#16
	
	vshr.u64	   d1,d0,#8	        ;d1:  p1 		p2		p3		p4 		p5 		p6 		p7 		xx
	
	;vshr.u64       d10,d1,#8       ;d10:  p2		p3 		p4 		p5 		p6 		p7	    xx      xx
	
	
	vshr.u64	   d12,d0,#56
	vext.u8        d10,d0,d12,#2
	
	
	vshll.u8        q1,d0,#0        ;q1:  p0 		p1		p2		p3 		p4 		p5 		p6 		p7				
	vshll.u8        q2,d1,#1        ;q2:  2p1 		2p2		2p3		2p4 		2p5 		2p6 		2p7 		xx		
	
	vaddl.u8        q3,d10,d11
	
	vadd.u16		q4,q1,q2        ; p0+2p1
	vadd.u16        q4,q4,q3        ; p0+2p1+p2+2
	
	vshrn.u16       d20,q4,#2	
	
	vst1.u32        {d20[0]},[r1],r3    
	vshr.u64	    d20,d20,#8		
	vst1.u32        {d20[0]},[r1],r3 
	vshr.u64	    d20,d20,#8		
	vst1.u32        {d20[0]},[r1],r3 
	vshr.u64	    d20,d20,#8		
	vst1.u32        {d20[0]},[r1],r3 
	
	ldmia       sp!, {pc}
	
	ENDP
		ALIGN 4
DATA
    DCD     0x02020202
    DCD		0x02020202
DATA2
    DCD     0x00000004
    DCD		0x00000000
		END